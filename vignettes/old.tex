\documentclass[draft]{article}

\usepackage{apacite}
\usepackage{bm}
\usepackage{geometry}
\usepackage{graphicx}
% \usepackage[none]{hyphenat}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{nth}
\usepackage[group-separator={,}]{siunitx}

\graphicspath{{.}}

\geometry{letterpaper}

%\linespread{2}

\title{Breaking boundaries: a review of contemporary methods for generating spatially explicit population models and their implications for research and policy}
% \shorttitle{} %apa only
\author{Chandler Armstrong, Marina Reilly-Collette, and Marissa Torres}
% \affiliation{} % apa
\date{} % delete this line to display the current date

\bibliographystyle{apacite}

\begin{document}

\maketitle
\newpage

% revised spin: some types of disasters require gridded data for assessment (eg hail vs earthquakes).  Some groups of people are more vulnerable  and when assessing we also need to know about the demographic characteristics of the population.

\begin{abstract} % need to summarize my treatment of boundaries too
  Spatial demographic data is the defining property of geodemography, and often a necessary component for policy and decision support.  Techniques for associating demographic data with spatial locations range from aggregating statistics to a predefined boundary% , vis-\'{a}-vis choropleth mapping,
  to complex areal interpolations and dasymetric mappings.  The physical size of spatial demographic data and the many different needs of its users have made it difficult to develop generalized methods and best practices for this common task.  Scientific researchers demand best estimates and diagnostics of uncertainty, whereas policy and decision makers might sacrifice accuracy for timeliness or simplicity.  % The following article highlights % a range
% methods for mapping demographic data.  %, discusses the spatially entrenched nature of policy, and argues for the importance of spatial demographics for policy making.
  The following review provides a brief technical comparison of two methods that underlie most techniques for downscaling spatial data: kriging and regression weighted dasymetric mapping.  The comparison focuses upon the aspects most relevant for demographic data, and draws upon literature to argue that regression weighted dasymetric mapping is, usually, a better option for spatially disaggregating demographic data.
  The discussion section
  % reviews scenarios from history that emphasize the long term impacts between spatial relationships and policy, and
  provides a short example of how the technical methods reviewed in this article can be applied to a practical problem of assessing social vulnerability to disasters around Metro Manila.
  The concluding sections argue that spatial disaggregation of demographic data is under researched and under utilized, and highlights topics and endeavors that can improve the methods available for accomplishing this task.
% policy should opt for more accurate methods, even when under the high-pace demands of many decision-making environments, and that contemporary methods and computing infrastructure can supply data that is accurate, simple, and timely.
\end{abstract}


% see DHS spatial analysis report number 9 for requiements by policymakers of downscaling maps.
% tobler79 and reibel07 (who cite gotway and young 2002) for the need to use data across incompatible spatial units, or to combine data from two different censuses.
% barrozo, also, for the need to disaggregate data to improve analysis of health

% Eicher and Brewer, in a similar assessment, point out that dasymetric mapping and areal interpolation are valuable for homogenizing spatial data aggregations and repartitioning aggregated data to arbitrary areal divisions.  % We intend to update E\&B's review, which itself was an update of a now 50 year old review of these methodologies.  In addition to updating the review, in this paper we also intend to more directly discuss how advancements in remapping technologies might be of particular benefit to decision-makers.

% kriging to quantify uncertainty of existing classifications:
% bruin 2000 Predicting the Areal Extent of Land-Cover Types Using Classified Imagery and Geostatistic
% freeman 2006 Evaluating Kriging as a Tool to Improve Moderate Resolution Maps of Forest Biomass
% silva 2013 Geostatistics and remote sensing methods in the classification of images of areas cultivated with citrus

% check out orford17 regarding policy
%           singleton regarding geodemography

% birkin-2011 has a lot of information on geodemographic mapping

% define dasymetric map in this paragraph
Maps of the population are essential for human geography, but spatially harmonizing, characterizing, subdividing, and discretizing population data remains a difficult task.  The simplest population maps are based upon demographic data that is subdivided into administrative boundaries--e.g. municipalities, provinces, or states.  Maps that aggregate data into predefined boundaries are called {\em choropleth} maps, and for many decades have been an essential tool of human geography.  Other types of spatial data can be partitioned into cells on a regular grid--i.e. a raster--or divided by contour lines; examples include population density, land cover, elevation, and weather metrics.  Raster and contour data are fairly simple to aggregate into an arbitrary set of spatial divisions \cite{tobler79} and, moreover, facilitate spatial and topographical analyses.  % examples?
Choropleth maps of population demography, on the other hand, are inconvenient for analysis; different data sets % and organizations (each use a distinct set of admin boundaries)
may use different administrative boundaries, and the aggregations of data within boundaries may do a poor job of representing true demographic contexts across large portions of the map.  
For these reasons, methods that redistribute demographic data from a choropleth map into a contour map or a regular grid are valuable tools for spatial and multiple variable analysis that incorporates data from both demographic and geographic domains.
% note: extend (or new next para) that anticipates conclustion (reimagining boundaries, human action and boundaries)
% It seems clear that a regular grid is advantageous for combining multiple spatial data sets, and the need to combine data from different organizations is a practical and frequent problem.  Therefore, methods for remapping demographic data to a regular grid of population density data facilitates spatial and multiple variable analysis that incorporates data from both demographic and geographic realms.

While rasters and contour maps bear many benefits for spatial analysis, it remains true that discrete spatial divisions are also valuable.  We identify two.  First, discrete boundaries can represent unmodeled aspects of the data that are important to consider but difficult to observe or compile (see for example \cite{birkin12}).  A geographic location may exhibit clustering of a particular type of household, for example, due to nearby jobs or services; we may observe the clustering without knowing the reason for its existence.  Nonetheless, a discrete boundary around the cluster classifies households within its boundary as similar, and can act as a so-called dummy variable that controls for all unknown and omitted variables associated with the grouping (for more information on dummy variables and omitted variable bias see \cite{wooldridge10}).  A second benefit is that decision makers also benefit from discrete groupings of data.  A raster or contour map, while computationally useful, might be more difficult for a human to analyze and use for developing and enacting policy or courses of action.

We propose that geographical boundaries are both limiting and necessary.  From this premise, the following article reviews the long-term importance of boundaries for decision-making and policy, and brings together a set of methodologies designed to spatially disaggregate demographic data and remap the geographic boundaries that demarcate the populations.  The effort is complex in scope, but the composing methods and ideas are each simple and self-contained.  We will demonstrate that a set of exclusive spatial subdivisions based on data rather than political administrative areas are simple to produce and simple to use.
% In this article we review methods for remapping and characterizing tabular demographic data, discuss the role of boundaries in shaping human activity, and offer conclusions for how different technical methods of analyzing and establishing spatial divisions of the population might influcence policy-making and the impacts of policy upon populations.

Geopolitical boundaries are often somewhat arbitrary in nature, and aggregations within these boundaries may not accurately reflect demographic realities on the ground.  Policy is often bound to whatever administrative divisions already exist, even if these divisions do not optimally represent social contexts within their borders.  Administrators and politicians, however, sometimes redraw boundaries to better suit their needs.  Counties and municipalities occasionally annex locations from neighboring administrations in order to bring newly urbanized areas within the jurisdiction of existing governance structures.  Politicians redraw political districts on a regular basis, sometimes to strategically dehomogenize or over-homogenize the districting, vis-\'{a}-vis gerrymandering, in order to shore up political power.  While administrative boundaries are semi-static in nature, decision-makers of all sorts will develop alternative boundaries when they recognize the benefits of doing so.

Methodologies that redistribute data from administrative divisions of a choropleth map into contours or a dasymetric map provide the foundational data for drawing supplementary boundaries that better represent types of people and the geographic space they occupy.  In this article we present methods appropriate 
% and discussion that will provide technically rigorous methods of
for downscaling demographic data and spatially subdividing populations into more homogeneous divisions.  We hope that such an improved spatial understanding of the population will assist scientists and policymakers in developing courses of action that are more appropriate for the affected populations, and more globally beneficial for all.  % While our intended audience is scientists and decision makers,
% The general public also benefits from a greater understanding of how their own spatial distribution is divided into bounded areas, in order to critically assess the decisions made by leaders.

% The review builds upon earlier % more than just eicher and brewer now
% comparison of dasymatric and interpolation methods for remapping population and housing density \cite{eicher01}.  In that article, Eicher \& Brewer compare the performance of five classic dasymetric and interpolation methods for their performance in remapping densities of total population, African-American population, and housing units by price bands.  We build upon the results of Eicher \& Brewer with a comparison of methods that are better suited for the particular task of remapping general demographic data.  Since Eicher \& Brewer's publication, dasymetric mapping of population density has become more accurate and standardized.  % --e.g. LandScan and Gridded Population of the World \cite{}.  % define topo?  embelish research enabled (dense urban, etc)? give examples of how it addresses.
% discuss uniqueness and difficulties of remapping demographic data?
% important: these methods suitable for global data (with sometimes poor spatial resolution)

% REMOVE ME candidate
Our discussion of the impacts of boundaries will highlight the ways in which policy takes on a life of its own within and beyond boundaries.  The discussion will furthermore reveal types of geospatial boundaries that may not exist on a map, but that can be nearly tangible in the lives of the people who must encounter these boundaries on a daily basis.  %In addition to unmapped socio-cultural boundaries,
Geophysical boundaries--e.g. watersheds and climatological areas--also intersect the geopolitical divisions that humans use to organize and implement policy.  Geophysical processes can carry the impacts of a policy implemented in one location to neighboring political divisions.  Time, also, often is unable to contain policy.  Policy enacted within a boundary can have an impact long after it has expired, and even after the boundary is erased or redrawn.  In these ways policy is both perpetual and transcendent.  Policy influences a generation of people, and has a long term effect on the lives of people and their descendants, many years after the policy and boundaries have changed.
% Geopolitical boundaries rarely encompass geophysical dynamics that intersect a boundary, and that can carry the impacts of policy beyond boundaries.

The benefits of our analysis might be summed up as {\em dissolving and transcending administrative boundaries}.  The discussion will move beyond administrative boundaries in two ways:  first, we expresses all data on a regular grid, which will facilitate seamless multivariable analysis; second, we identify boundary lines based not on administrative divisions, but rather geodemographic arrangements.  The goal of this paper is to review and reveal new and analytically useful methods for subdividing and describing population.  

% discuss hyperspectral analysis?
% discuss the anonymity of the data?

It bears mentioning that demographic data is generally aggregated to administrative or enumeration boundaries for purposes of privacy and anonymity, and that the purpose of disaggregating the data is not to rediscover who lives at what address.  As scientists and decision makers we want higher resolution data than the aggregations provided by most population surveys, but we (hopefully) wouldn't use data with household identities, even if we had access to it.  Disaggregation procedures, both dasymetric and interpolative methods, estimate rates or probability densities over areas of space; the likelihood that a randomly selected unit--e.g. person or household--possesses a given characteristic or set of characteristics.  As such, these methods maintain the anonymity and privacy of people who live in the places we study.  Even if we are able to dissaggregate the data to a very high resolution, say 30 meters, the results tell us only the probability that a random person with a given attribute might exist in that 30 meters within some period of time.

% Where population density may have some uncertainty, it is more-or-less deterministic; the values are fixed and change only with the underlying data.  Spatial demographics, on the other hand, are fundamentally random; these variables do not represent physical densities, they represent probability densities.  Spatial demographics describe what kind of people one is likely to meet in a given spatial population, and the probability of meeting each one.

The paper will proceed as follows.  The first section will briefly review the methods required to redistribute data and identify meaningful boundaries.  The second section will provide historical examples of the power of boundaries to shape society through time.  The third and final section will discuss how data-driven boundaries can assist policy and decision makers, and provide a motivating example.


\section{A review of methods}

Our discussion of mapping and rebounding demographic data is anticipated and motivated by three broad areas of research.  % that, when brought together, form the base components of an analysis of the spatial distribution of population.
These areas of research are 1) geodemographics and spatially explicit population models (SEPMs), 2) dasymetric mapping and interpolation, and 3) spatial population connectivity.  Geodemographics and SEPMs refer to the end product of interest, and is where the review begins.  Dasymetric and interpolation methods are how we will produce our SEPMs, and are covered in the second section.  Population connectivity is a nebulous concept that encompasses intrinsically spatial aspects of population, including distances and divisions, and provides inputs for dasymetric modeling and guides drafting of new analytical boundaries.  Important details on each of these three aspects is provided in the following subsections.


\subsection{SEPMs \& geodemographics} % \& dasymetric mapping

% Generating SEPMs is a complex topic motivated by cross-discipline research findings.  This section will review motivations for accurate SEPMs within policy and science.

Spatially-explicit population models (SEPMs) combine population simulation with a landscape map \cite{dunning95}.  By marrying computational simulation with ecological analysis, SEPMs investigate questions that conventional techniques can not, and provide the ability to simulate population response to regional or global change (see for examples \cite{levin92, ray09, chandler13}.)  SEPMs facilitate inclusion of spatial with population information, and offer researchers the ability to develop and test more complex rules for how animal populations, including humans, move, disperse, and behave in space.

One of the earliest surveys of an urban area that also included a high-resolution SEPM %spatial demographic analysis
was published by Charles Booth in the multivolume work ``Life and Labour of the People of London'' \cite{booth1892}.  Booth supplemented the 1891 UK census with ethnographically collected data in order to map out and color-coded classes of wealth and poverty across London on a street by street basis.  Booth's analysis marked the origin of geodemographics, and also provided important data that Booth used to push for social reforms including pension programes and free school meals for children of poor families.  Geodemographics is the study and classification of spatial population segments by demographic characteristics.  Geodemographics today retains it's highly applied intents and mixture of methodologies, and also has developed into a vital aspect of market, political, and policy research.  %, where geodemographic classifications are used for targetting advertisements and products to the most desired demographic segmentations.

Geodemography is a type of SEPM that classifies spatially subdivided populations into broad classes.  For example, Patchwork Nation \cite{pwnation} is a geodemographic tool that classifies US counties into categories such as ``tractor country'', ``emptying nests'', or ``campus and careers'', to name only a few.  Open Geodemographics is another tool, similar to Patchwork Nation, that provides hierarchical geodemographic classifications for residential areas across the UK \cite{oacug}; classifications of the top level hierarchy include ``affluent england'', ``countryside living'', and ``cosmopolitan London''.  Another geodemography tool serving the UK is ACORN (A Classification of Residential Neighborhoods) \cite{acorn}, a closed source geodemographic tool designed for market research.  Geodemography is distinct from SEPMs more generally by its intent; with broad classification of discrete spatial subdivisions geodemography provides a tool for analysts and practitioners to locate, reach, and act upon populations.  A SEPM is simply a model, whereas geodemography is a theory-based analysis of a SEPM designed to answer questions relevant for deciding upon courses of action.  %  The distinction is important because we will be discussing how methods of generating a SEPM might impact policymaking.
% singleton-2014-the past, present and future of geodemographic research in the US and UK

A particularly extreme type of SEPMs are those that localize people in a population to exact coordinates within an area--we call these synthetic population models (SPMs), see for examples SPEW \cite{gallagher18}, Gen* \cite{chapuis18} or simSALUD \cite{tomintz13}.  Where most SEPMs strive to provide a map with greater regularity and a greater degree of accuracy than that available in a choropleth map, a synthetic population model attempts to provide data that can be used to simulate individual agents, and is suitable for agent-based modeling and similar methods.  SPMs are valuable for problems requiring simulation of individuals; examples include transportation modeling \cite{barthelemy13}, infectious disease modeling \cite{xu17}, \cite{gallagher18}, and modeling the impacts of job loss and gain on local areas \cite{ballas01, ballas06}.  The exact localizations of SPMs are difficult to verify or analyze in terms of their uncertainty, however, and most SPMs depend only upon visual analysis to assess that the localizations look plausible.  % Given this limitation and the fact that such exact models of the population are typically unecessary for most policy and decision-making, we do not discuss SPMs further in this review.

Raster and contour-based SEPMs occupy a middle-ground between those that classify populations within pre-defined areas, such as census-blocks or streets, and those that simulate individual agents of the population.  A raster-based SEPM uses ancillary data to classify population within regularly sized cells, for example a SEPM that classifies mean housing prices across a grid of cells, each sized one square kilometer, based upon the land use within the cell (see for example \cite{eicher01}.  A contour-based SEPM interpolates population data between known points, for example a SEPM that interpolates housing prices between known points of recent sales (see for example \cite{montero11}.  The next section will discuss these two types of SEPMs, and the methods that produce them.


\subsection{dasymetric \& interpolation methods}

% Remapping methods can be categorized into two broad types: dasymetric mapping and interpolation.  These topics address key methodologies that enable a wide array of geospatial population research.  % this review builds upon a host a previous reviews, while making a couple of distinct contributions.  First...

Redistributing data between two different areal zones is the {\em change of support problem} \cite{cressie96, gelfand01}, and it is typically solved with areal interpolation (see for example \cite{montero10, montero11}) or dasymetric mapping (see for example \cite{mennis03, bentley13}.)  Dasymetric mapping uses ancillary data to identify spatially homogenous areas, it contrasts with interpolation methods that instead make use of spatial auto-correlation and depend upon assumptions about the smoothness of data across boundaries.  The accuracy of either method depends upon an ability to identify spatially homogenous areas \cite{maantay07, maantay08}.  Essentially, if the ancillary data is a better indicator than spatial auto-correlation for identifying spatial homogeneity, then dasymetric modeling will outperform areal interpolation, otherwise interpolation is a better option \cite{nagle14}.  Both types of methods are very broad, and the precise techniques employed vary greatly within each family.
% Choosing between these two families of methods, dasymetric and interpolation, is not a simple choice, and should be based upon the available data and how the resulting remapped data will be used.

Examples of interpolation % of socioeconomic & demographic data
include population isopleths \cite{schmid55}, pycnophylactic interpolation \cite{tobler79}, and area-to-area/area-to-point kriging \cite{krivoruchko11, kyriakidis04}.  Kriging methods are more commonly associated with geophysical data, where block kriging \cite{cressie90, matheron63} is the most common form of kriging and is used to estimate volume of some substance over an area from point samples, for example volume of oil or minerals.  Interpolation of population data, however, seems to have been the first application for geographic interpolation.  
Schmid \& Maccannell \cite{schmid55}, publishing in the mid \nth{20} century, appear to be the first to formally address the techniques and problems of interpolating population measurements.  
Tobler \cite{tobler79}, publishing in 1979, mentioned that Schmid \& Maccannell was the only previous literature to highlight the desirable mass-preserving, or pycnophylactic, property of interpolation.
% please expand this paragraph
% The original motivations for interpolation of population were to summarize data, smooth out small-scale heterogeneity, and facilitate reaggregation of data to arbitrary bureacratic partitions \cite{schmid55, tobler79}.  
% Without interpolation, population maps are essentially 3D histograms.  The interpolated data may impose errors of false precision, nonetheless % Schmid \& Maccannell conclude that a smoothly curving population density surface is a better representation of empirical observations and more useful than a 3D population histogram.

% Population isopleths can be arbitrarily precise, they are hypothetical constructs representing rates and not a real measurement at a point in space, hence isopleth lines give a false sense of precision.
Despite historical precedent, interpolation of population data is questionable.  Population is a rate and, unlike a mineral sample, not a real measurement at a point in space, hence population isopleths can give a false sense of precision.  Interpolation assumes measurements vary smoothly over space, they in essence depend upon Tobler's first law of geography: that near things are more related than distant things.  This assumption, however, is often unjustified for population, where density of population or its demographic characteristics can change sharply at arbitrary geographic boundaries \cite{schelling71}.  Demographic data are attached to discrete {\em objects} and can not sensibly be viewed as a {\em field} (see \cite{cova02, kjenstad06} for discussion of the distinction betwenen spatial fields and objects.)  % o'sullivan 2010 geographic information analysis
Interpolation methods may be valuable, nonetheless, where point data must be combined with areal data (see \cite{gooaverts10} for an example from medical geography.)  ESRI tested area-to-area kriging when applied to demographic data and found the results to generally be within acceptable limits of accuracy \cite{carson13}.  Amos et al \cite{amos17} compared area-to-area and area-to-point kriging with dasymetric mapping--which we will dicuss shortly--for the purpose of distributing census data into voting precints that are split by a census block; their results found that dasymetric mapping was superior for this application, and that kriging estimates often fell outside of recommended error bounds.
% gooaverts, gotway & young, kyriakidis, amos, carson for discussion and evaluation of kriging for demographic data
% see amos page 392 for discussion and citations on the problems of interpolation and kriging applied to population data.
% In the context of sociodemographic modeling, smoothing methods may be dubious because demographic data typically is not smoothly varying; for example gender or income cannot sensibly be said to have some value at all points in space, there can exist sharp spatial disconuities in things like socioeconomic status, and the spatial variables that explain these discontinuities are difficult to observe and, at any rate, are likely endogenous.
% reibel also points out that interpolation methods make an assumption of smooth variance that is often unsupportable where administrative and political boundaries coincide with real geographic boundaries.

% Contour lines of a density are known as {\em isopleths} (equal-values) because the lines represent equal rates or ratios, whereas contour lines of absolute measurements, like elevation, are known as {\em isometrics} (equal-measures).  Another way of describing the difference is that isopleths represent areal data (rates) whereas isometrics represent point data.  Contour lines and metrics of terrain summarize and interpolate spatial data and facilitate classification of population features; e.g. ranges of population density ``mountains'' or areas of density ``roughness''.

Dasymetric mapping was introduced to U.S. audiences by J.K. Wright in 1936 \cite{wright36}, but the method was developed properly in 1911 by Benjamin Semeonv-Tian-Shansky and methodologicall anticipated nearly a century earlier still by Poulett Scrope and Henry Drury Harness \cite{petrov12}.  The method has a long history, likely because it is simple to apply yet consistently performs well for the task of redistributing spatial data \cite{eicher01, holt11, barrozo16, amos17}.

Dasymetric mapping methods can be broadly categorized into three types: area weighted, homogenously weighted, and regression weighted \cite{reibel07}.  Area weighted is the simplest of the three, and simply reportions data between a source and target areas based on the respective size of each target area within the source area (see \cite{goodchild80} for an example).  Homogenous weighting searches for source zones that are completely covered by each category available in the ancillary data, and uses these zones to estimate the population density for that given category (see for example \cite{eicher01, mennis03}).  Regression weighting regresses population counts from source zones upon the area of each ancillary category within the source zone to obtain population density weights (examples are \cite{flowerdew89, flowerdew92, reibel07}.)  By fitting ancillary data to population counts using a Poisson distribution, the coefficients for each ancillary category may be interpreted as the  population density of that particular category \cite{reibel07}.  Regression weighting might be the most sophisticated of the three dasymetric methods, but with modern software it is simple to apply and interpret, and avails any analysis of a vast array of regression methods.
% I repeat the word "ancillary" too often

Kriging and regression weighted dasymetric mapping (RWDM) both employ regression methods.  Kriging analyzes covariance between data points, and is generally equivalent to a gaussian process (GP) regression model \cite{rasmussen06}.  Regression weighted dasymetric mapping analyzes covariance between variables, vis-\'{a}-vis ordinary least squares or generalized linear modeling.  For both methods the data points are similar or identical: a column vector of measurements $\bm{y}=\{f(\bm{x}_i), ..., f(\bm{x}_n)\}=f(X)$ at points in space $X=\{\bm{x}_i | i=1, ..., n\}$.  For kriging the values of each $\bm{x}_i$ are typically coordinates, and $y_i$ is measurement at those coordinates--e.g. a volume of mineral, inches of rainfall, or height of a plant.  For RWDM $\bm{x}_i$ is a vector of independent variables and $y_i$ is again a measurement associated with $\bm{x}_i$.  Both methods begin with $n$ inputs $\{(y_i, \bm{x}_i), ..., (y_n, \bm{x}_n)\}$, and both methods essentially derive a vector of weights for predicting an unobserved value $y_*$.  Kriging weights is a row vector $\bm{w}_*$ of $dim(\bm{w}_*)=1 \times n$ and an unobserved location is estimated as $y_*=\bm{w}_*\bm{y}$.  Regression weights is a column vector $\beta$ of $dim(\beta)=p \times 1$, where $p$ is equal to the number of variables in each $\bm{x}_i$, and an unobserved location is estimated as $y_*=X\beta$ % For kriging it is essential that $\bm{x}_i$ at least contain spatial coordinates, whereas for RWDM $\bm{x}_i$ may contain only the ancilliary data associated with the measurement of interest $y_i$.
While their inputs are similar, how kriging and RWDM make use of that data is quite different.  The following paragraphs give equations for ordinary kriging and ordinary least squares regression to algebriacally illustrate the differences between gaussian processes regression and general(ized) linear models.

Before launching into this technical comparison, we must note that implementations of kriging or regression weighted dasymetric mapping will often use a variant of these methods that is specialized for the task at hand.  Kriging implementations designed for use with population data will generally use area-to-point or area-to-area kriging.  Likewise, RWDM ought to model a Poisson or binomial distribution (for population counts or proportions, respectively.)  The following paragraphs should not be taken as a description of an implementation.  They, instead, describe the fundamental algebra for the simplest point kriging and OLS regression in order to convey the basic principles of the methods and, more importantly, the differences between them.

% $(y_i, x_i)_{i=1}^n$
Kriging assumes that the observed data $\{y_i, ..., y_n\}=\{f(\bm{x}_i), ..., f(\bm{x}_n)\}$ is a single sample from a multivariate gaussian process at locations $X=\{\bm{x}_i | i=1, ..., n\}$ %\{\bm{x}_i, ..., \bm{x}_n\}$
with a constant mean over the sample area, and builds three matrices from the data.

\[K=
  \begin{bmatrix}
    k(\bm{x}_1, \bm{x}_1) & \cdots & k(\bm{x}_1, \bm{x}_n) \\
    \vdots                & \ddots & \vdots \\
    k(\bm{x}_n, \bm{x}_1) & \cdots & k(\bm{x}_n, \bm{x}_n)
  \end{bmatrix}
\]

\[K_*=[k(\bm{x}_*, \bm{x}_1), \cdots, k(\bm{x}_*, \bm{x}_n)]\]

\[K_{**}=k(\bm{x}_*, \bm{x}_*)\]

The function $k$ is the covariance function--or kernel, in GP terminology.  In addition to first order stationarity--a constant mean--kriging also assumes second-order stationarity--constant covariances--so that all data points share the values of these matrices.  The first matrix $K$ is the $n \times n$ covariance matrix for all observations.  The second matrix $K_*$ is the $1 \times n$ row vector of variances between the coordinates $\bm{x}_*$ and those in $X$.  The third and final matrix is the maximum variance $\sigma_{f}$ of $y_*$ at $f(\bm{x}_*)$, and because we assume a stationary second moment it also the maximum variance of all the measurements; therefore it is equal to the values on the diagonal of $K$.  Although the covariances of $K$ are functions only of the values in $X$, the kernel $k$ has a number of free {\em hyperparamters} (e.g. $\sigma_f$, the maximum variance of $\bm{y}$) that must be estimated from $\bm{y}$.  In geospatial sciences, the model parameterizing the kernel is often called the (co)variogram.
% does variance depend on y, or isn't it same for all?

The probability of a particular estimate for $y_*$ is given by 

\[y_* | \bm{y} \sim N(K_*K^{-1}\bm{y}, K_{**}-K_*K^{-1}K_*^T)\]

% I mention unbiasedness, but not minimum variance...
where the best estimate $\hat{y}_*$ for $y_*$ is the mean parameter of $N$ (the first parameter.)  The kriging weights of the best estimate are $\bm{w}_*=K_*K^{-1}$.  Notice that the kriging weights depend upon the location $\bm{x}_*$ via $K_*$, and weight the impact of $\bm{y}$ upon the estimate $\hat{y}_*$.  A consequence of the assumption of a constant mean over the sample area is that $E[\hat{y}_*-y_*]=0$, or that the estimate is unbiased.  Notice also that, as the covariance is stationary (the second parameter), it too doesn't depend on the values of $\bm{y}$.  The two most important aspects of these equations to understand is 1) the kriging weights only depend on the locations in $X$ and 2) they model covariance between these locations.  The next paragraphs will similarly introduce OLS, which differs from kriging in that it models covariance between the variables that characterize the data points.  For a fuller discussion of kriging that includes additional methods, derivations, and proofs see \cite{cressie93} and \cite{rasmussen06}.

% for both kriging and OLS it might help to give the logical prediction equation, that demonstrates the weights/betas.  this would allow me to introduce \epsilon and make it easier to discuss lack of bias and exogeneity.
% just mention key assumptions, and avoid comparing the assumptions (it may be out of scope) and focus on differences in how weights are estimated
Ordinary least squares makes similar assumptions as kriging for the process that generates the data $\bm{y}=\{y_i, ..., y_n\}$, but the similarities cease here.  OLS builds the data into a column vector

\[\beta=(X^TX)^{-1}X^T\]
% $P=X(X^TX)^-1X^T$

The best estimate for an unknown $y_*$ given input data $\bm{x}_*$ is

\[\hat{y}_*=\bm{x}_*\beta\]

with variance equal to

\[\hat{\epsilon}=\bm{y}-\bm{\hat{y}}\]
\[\sigma^2=\frac{\hat{\epsilon}^T\hat{\epsilon}}{n-p}\]

Here $X^TX$ is a matrix with $dim(X^TX)=p \times p$; it is similar to the matrix $K$, above, with a linear kernel, in which case $K=\sigma_fE[XX^T]$.

The column vector $\beta$ is somewhat analogous to the kriging weights $\bm{w}_*=K_*K^{-1}$.  The values of $\beta$ are associated with the effect of the variables in $X$, whereas the kriging weights quantify the effect of the observed values in $\bm{y}$ given their distance from the unobserved location $\bm{x}_*$.  This difference implies another one of importance: the values of $\beta$ are the same for any given $\bm{x}_*$, whereas kriging weights depend upon the values of $\bm{x}_*$.  A similarity between both kriging estimates and those provided by OLS are that all estimates are assumed to have the same variance, as dictated by kriging's assumption of second order stationarity, or what OLS calls homoscedasticity of variance.  These equations summarize only the algebra of how OLS derives the effects of inputs on outputs; for a fuller discussion on the assumptions, diagnostics, and variants of linear modeling, see \cite{nelder72, mardia80, gelman06}.

RWDM does not, in general, preserve the original volumes of the source zone data used to estimate values in the target zones--where in this case the target zones are grid cells of a raster.  For this reason, RWDM estimates should rescale the estimates so that, when summing across target zones within a source zone, the sums equal the original source zone data.  The equation to accomplish this is

\[\hat{g}_{i*} = \hat{y}_{i*} \frac{y_i}{\sum_{j=1}^{n_i} \hat{y}_{ij}}\]

where $\hat{g}_{i*}$ is a rescaled estimate within source zone $i$, $\hat{y}_{i*}$ is the raw estimate, $y_i$ is the observed value of source zone $i$, $n_i$ is the number of target zones that intersect source zone $i$, and $\sum_{j=1}^{n_i} \hat{y}_{ij}$ sums over all $j$ raw estimates in source zone $i$.

% $P$ is a projection matrix that will project the measurements in the column vector $\bm{y}$ to the hyperplane that minimizes the errors between the estimates $\bm{\hat{y}}$ and the observed values in $\bm{y}$; expressed compactly as

% $\bm{u}=\bm{y}-\bm{\hat{y}}=\bm{y}-P\bm{y}=(I-P)\bm{y}$

% Where kriging analyzes, even exploits, autocorrelation within the data, OLS must assume it doesn't exist.  OLS must also assume that the observations are independently and identically distributed (iid), whereas kriging is tolerant of non-iid data.

% but for accurate diagnostics OLS requires additional assumptions regarding the nature of the prediction vectors ${\bm{x}_i, ..., \bm{x}_n}$.  The key 
% These assumptions are strict exogeneity, linear independence of $\bm{X}$, homoscedasticity, no autorcorrelation, normally distributed errors, and independently and identically distributed observations.

The above brief summary comparison of kriging and regression weighted dasymetric mapping is designed to illustrate one important difference.  Kriging is a smoothing model that estimates values at unobserved locations based on distance from observed data and the autocorrelation structure among the observed data.  Regression weighted dasymetric mapping is a linear model that estimates values at unobserved locations based on the covariance of the attributes at the unobserved location with the known data, given the covariance structure of the known data.  This comparison is a bit unfair, given that kriging can incorporate additional attributes via regression-kriging \cite{hengl07}, also known as universal kriging \cite{wackernagel03}, or kriging with external drift \cite{bourennane00, hudson94}.  The point remains, nonetheless, that kriging smooths data based primarily upon spatial distance, whereas regression weighted dasymetric mapping depends primarily upon covariance with ancilliary attributes.  This is an important distinction for policy and decision makers, where similarity between observed and unobserved locations may depend more upon similarity of their attributes rather than similarity of their spatial location (however, for applications where the estimated data does vary smoothly over the sample area, regression-kriging clearly outperforms OLS \cite{bourennane00}).  RWDM, moreover, may be simpler to apply in practice since it is basically a general(ized) regression model, and doesn't require the intermediate step of developing a variogram.

% The final paragraphs of this section will discussion estimation and uncertainty quantification via an application that, we believe, is particularly appropriate for demographic classification: land use classification.  We will show that methods similar to RWDM are the precedent within land use classification; which suggests that RWDM may also be preferable for the purpose of demographic classification.
% Review of the literature also suggests that, for land use classification, kriging is more often applied {\em post hoc} to existing land use classifications as a means to improve spatial uncertainty quantification (see for example \cite{freeman06, debruin00}.)
% kriging doesn't do well with geodemographic classifications, because it works only with continuous data, whereas dasymetric mapping can yeild categorical classifications.

The final paragraphs of this section will discuss uncertainty quantification.
Despite their differences, both kriging and RWDM are fundamentally regression techniques, and both have similar options for % fitting the model and
quantifying uncertainty.  For our purpose of disaggregating demographic data, we wish to highlight a method that is often selected for non-spatial analyses of demographic data: the bootstrap.% (for examples see \cite{ghimire12, defries00}.)

The bootstrap was developed in a 1979 article by Efron \cite{efron79}, based on the similar but simpler jackknife.  The bootstrap estimates uncertainty by assuming that {\em resamples} from the sample data relate to the sample data in the same way that the sample data relates to the population.  Conventional parametric statistics usually quantify uncertainty by invoking the Central Limit Theorem, which states that the averages of infinite samples of size $n$ from a population will approach a normal distribution.  Hence the statistical estimtes--which are means--may be assumed to be drawn from a normal distribution whose parameters can be estimated from the data.  The bootstrap, instead, directly probes sample properties by drawing resamples.  The caveat of the bootstrap is that the resample must be generated in the same way as the sample.  If the cases in the data are independent from each other, then it's simple enough to replicate the sample with random draws; complex survey designs and dependent data, however, must be more thoughtful.
% describe here how bootstrap quantifies uncertainty

The basic bootstrap is not suitable for dependent data, including all spatial data, but in a 1989 paper K\"{u}nsch \cite{kunsch89} introduces the {\em block-boostrap} as a method for bootstrapping dependent data.  The block-bootstrap is actually similar to the older jackknife methods, and resamples blocks of cases rather than individual subjects.  % will need a previous jackknife citation and discussion to summarize jackknife; see kunsch
Under the theory of the block-bootstrap, blocks of cases retain their dependence structure.  A block of spatial data is simply a contiguous patch of land, e.g. a neighborhood of raster cells or all cases within an administrative division.

When employing the block-bootstrap, The blocks may be either overlapping or non-overlapping.  Non-overlapping block-bootstrap divides the space into square blocks, and resamples these blocks of data.  The overlapping version simply resamples from all original spatial data cases, and along with the sampled case also selects those from some neighboring area.  The size and number of blocks to resample is not always obvious, but some have researched methods for determining optimal block size (see for example \cite{nordman07}.)

As the goal is to spatially redistribute demographic data, the bootstrap could be applied to either or both of the demographic and spatial data.  The first option is to bootstrap only the demographic data.  This option resamples demographic data, stratified by source geographic zones if necessary, and fits the resampled data to the full spatial data.  The second option is to bootstrap only the spatial data.  This option requires preservation of the spatial dependence in the data, which is accomplished with the block-bootstrap, previously described.  The third and final option bootstraps both spatial and demographic data, and furthermore has two variants depending upon the order of the bootstrap.  One can first resample the demographic data, then resample blocks of spatial data, and proportion the demographic data between the blocks as usual.  Alternatively, one could first resample spatial blocks of data, then resample demographic data within each block, again with the demographic data proportionally stratafied given the area of each source zone within the block.  The second option would seem to be more thoroughly mixed, since the demographic data is resampled for each spatial block, rather than resampled only once and reused for all the blocks.

% is this necessary?
% Leo Breiman, in a 1994 technical report, propopsed a method for applying bootstrap methodology to machine learning models.  Breiman's technique generated $m$ resampled data sets, and fit a classifier to each resample.  The resulting classifications are then aggregated to an average classification, giving the technique its name: bootstrap-aggregating, or bagging.
% With the block-bootstrap, bagging can be used to classify dependent data, including time series and spatial data.  Indeed, bagging is among the most successful classifiers for population density estimation \cite{} and land use classification \cite{gomez16}.  % more examples
% next: bagging + block bootstrap, random forests, standard now in landuse and other spatial classifications.


\subsection{population connectivity}

This section reviews a final ingredient for our analysis of spatial demographics and policy: population connectivity--and by implication population boundaries.  The purpose of this section is to demonstrate that the boundaries and connections easily drawn upon a map do not always align with how structural factors have organized the world, nor with how people organize the world within their minds.  A secondary purpose is to review methodologies for drawing boundaries that better reflect reality as it is percieved by people living in the spaces represented by the maps. % that policy and decision makers use to enact courses of action.

This section is somewhat {\em posthuman}, in that it draws inspiration from analyses of both human and non-human animal activity.  Analyses of human movement is often more topological in nature, in that such analyses tend to employ theory that ignore space and distance.  A prime example are theories of migration.  Migration is, obviously, a spatial activity, however migration theories rarely explicitly consider space and distance, and instead analyze historical geo-political systems \cite{wallerstein74}, labour economics \cite{harris70, stark85}, and familial relationships \cite{macdonald64, myrdal57, massey90}; in all these theories space is largely irrelevant.  Indeed, space typically enters into models of human migration only as a means to circumvent endogeneity of human systems in the patterns of human movement.  The idea is that, {\em ceteris paribus}, humans will choose to migrate shorter distances \cite{woodruff07, garip12}.  Space is useful in this way because it is largely exogenous to human systems, but for migration theorists, it turns out, also of little sociological interest.  Theories regarding non-human animal movement and activity, on the other hand, are almost entirely spatial in nature, and analyze distance \cite{peer08}, topography \cite{peer06}, and elevation \cite{alderman07} to identify only a few.  The following paragraphs propose that a synthesis of these different families of theories regarding human and non-human spatial activity can lead to a fuller theory of human connectivity that will help us to encode upon a map the spatial connections and boundaries that people encounter in their lives.
% needs more examples of studies of animal movement: manel03

While this section occasionally makes use of migration as an example of connectivity, it is important to realize that connectivity doesn't entail a change of residence.  Connectivity may also refer to local access to goods, services, and other resources.  If local access fails, then a family may choose to send members abroad to obtain these resources elsewhere.  For example, if a family is unable to access employment or credit locally, then some members may migrate to find these resources in a nearby city or another country, and send back those resources in the form of remittances.

% Based upon Weiss' map of travel times, Linard et. al. \cite{linard12} analyze accessibility to resources by populations across Africa.
A recent example of a theory of human movement that is primarly spatial comes from Weiss et al \cite{weiss18}, ``A global map of travel time to cities to assess inequality in access to resources''.  Weiss et al argue that access to cities is a major factor of economic opportunity and sustainable development.  Travel time maps support higher-level analysis of population distribution, settlement, and access to resources.

Linard et al \cite{linard12} identify three spatially-explicit aspects of population connectivity: density, distance, and division.  Density refers to spatial concentration of people and resources. Density brings people and firms closer together, and facilitates exchange of knowledge and collaboration.  Abel el al \cite{abel12} find that population density increases economic productivity by 2 to 4 percent, even after controlling for endogenous relationships due to self-selection by more productive individuals and firms who select to move into high density locations.  Distance, for Linard, is a function of spatial mobility and access, not of conventional Euclidean distance.  Linard's definition of distance echos Weiss et al's so-called friction surface used to calculate travel times.  The friction surface model, moreover, resembles earlier models used to simulate gene flow between non-human populations, in particular the theories of isoloation-by-distance \cite{ishida09} and isolation-by-resistance \cite{mcrae06}.  Division is an inverse  measure of the spatial integration of society's productivity, where integration is assessed via the friction surface definition of distance.  Examples of integration include proximity and access of agriculture to markets, and access to healthcare by populations.
% more examples for division/integration in linard citations 21-24
% Linard emphasizes that population demographic heterogeneity alone is not sufficient to model human spatial mobility and socio-economic structure, and offers the need to measure population characteristics relevant to the economy at a scale more discrete than the heterogeneity of the population.
% A component of this analysis is the ``Average per-person travel time to the nearest settlement with more than 50,000 people, calculated by combining the global map of accessibility with our detailed population distribution dataset'' \cite{linard12}. Global maps of accessibility are based on street maps, google earth, and other analytical tools of roadways, mass transit, etc, for walking and mobility, and have been worked on by Uchida \& Nelson \cite{uchida08} and by Weiss et. al. \cite{weiss18}.

% These maps allow for difficulty of accessing resources to be properly understood on a global scale. However, they assume foot or vehicle mobility occurs along GIS-referenced roads or mass-transit systems. This is still an immensely powerful tool, as roads and mass transit account for the vast majority of trips. But especially for the very poor, they are an incomplete vision, and as we shall see, assume that everyone is equal in perception of ease of access.


% Landscape genetics is the study of how geographical and environmental features structure genetic variation at the population and individual levels \cite{manel03}.
As previously mentioned, travel time maps bear a resemblance to animal movement models, in particular Isolation-by-Distance (IBD).
IBD models movement between {\em demes} as a function of distance and migration probabilities \cite{ishida09}.  More recent developments along the lines of IBD theories have produced isolation-by-resistance (IBR) \cite{mcrae06} and isolation-by-environment (IBE) \cite{wang14} models.  IBR analyzes not only distance, but also the resistance of intervening landscape to movement--which is similar to Weiss' friction surface model used to calculate travel time to cities.  IBE extends IBD by also considering environmental differences of the {\em demes}, for example humidity and elevation.
% SEPMS explicitely model the landscape that intervenes {\em demes}, and allow for distance to be measured as a landscape cost rather than simple geographic distance.

IBR and IBE are apt anologies for the issue of human accessibility to cities and their resources.  Although human accessibility may not pertain to gene-flow, per se, difficulty of traveling across some distance and socio-cultural differences between two locations drive our suggestion that it is possible and fruitful to map socio-cultural boundaries.  Poor infrastructure may render a relatively short distance impassable.  Lack of access to vehicles can also act as a sort of boundary to those who can't afford a car or airfare.  Differences in language, class, or social ettiquette mirror the environmental differences of IBE, and also act to impede access to resources that may otherwise be within travelling distance.

% not modeled: interference from other individuals

Mobility itself is about more than roads, since humans (as well as animals) have to deal with movement in spaces beyond the gridded network topology of roads and mass transit links. In this area  Guy Pe'er emphasizes individual response to topography and defines the concept of topographical ``noise'' \cite{peer06}. Guy Pe'er and Stephanie Kramer-Schadt further cover % in ``Incorporating the perceptual range of animals into connectivity models'' \cite{peer08}
the importance of perception of topography--an elaboration of individual response \cite{peer08}. Taking these works as inspiration for analogy, we consider that, for humans too, the connectivity of populations is inherently linked based on perceptual rather than actual distances, suggesting that rather than a strict analysis of travel times, analysis of connectivity must incorporate perceptual difficulties in movement as well.

% the metapopulation is an example of IBE
Collectively, these characteristics of a linked population forming a series of islands separated by areas of resistance to habitation or mobility, follow well with the concept of a metapopulation. Ilkka Hanski championed the concept of a metapopulation and of a spatially-realistic metapopulation theory \cite{hanski01}, which was an evolution of a classical metapopulation theory summarized by Hanski in \cite{hanski91}.  Metapopulations seem to echo the concepts behind IBE.  The basic assumptions of this type of theory are important for considering human connectivity in the context of physical, social, and cultural landscapes.  
% Perhaps most human groups are best studied as metapopulations, because the characteristics defining a human community are unlikely to exist in only one place.

An example is appropriate to illustrate. It is almost certain that a particular neighborhood (for instance, Rusyn immigrants in Pittsburgh in the 1950s who are working class and employed at steel mills) will have a population essentially identical in the quantified terms, but separated in some way by physical topography, including spatial distance (Rusyn immigrants in Cleveland in the 1950s who are working class and employed at steel mills). The two Rusyn communities in question are likely to exchange priests, sharing the Greek Catholic religion of their homeland, and are likely to meet at events at which they have an opportunity to intermarry. The communities are separated during the age when white working class ethnic identity was still largely self-segregated in the northern industrial citiies, but are still linked in a real exchange.

% This directly relates to the definition of a metapopulation, as in ``Metapopulations and Spatial Population Processes'' by Ilkka Hanski \cite(hanski12}. Though concepts related to genetic variation are not relevant in humans due to relatively high interconnectedness, ethno-national and linguistic variation may occupy a similar space, suggesting that the incorporation of cultural factors into population topology is an important addition to the physical topologies defined above.

% this is(was) a good concluding paragraph for this section
Humans have myriad ways of perceiving the environment, both strictly physical and mediated by cultural factors. In India, knowledge of a caste difference between residential areas may serve as an effective block against seeking resources (water, food, or employment) as much as visually perceptible changes in terrain. Therefore, a population topography from an Urban Planning perspective may in fact incorporate any potential factor, both physical and cultural, that impact human movement. Two human populations are often separated by a river, a railway, or a superhighway, but potentially also by ethnic segregation in an urban area. In this sense, human inhabitation is directly linked to concepts of habitation fragmentation and landscape connectivity.

% wong12: Bringing the person back in.  Avoiding MAUP by drawing aggregation boundaries based on people's percieved contexts.  Admin boundaries do not accurately reflect who people organize the world in their heads.  To the extent that people's perception of their own context is relevant for policy and decision making, redrawing aggregation boundaries based on people's context would also benefit policymakers.  My method redraws boundaries based on statistical notions of homogeneity, is this also beneficial, or does it possibly align with or tell us something about people's contexts in their heads?  The article finds people's ideas of their own community are different than the administrative boundaries used by government.

% wong15: Mapping policies and programmes: the use of GIS to communicate spatial relationships in England
% One goal of analyzing connectivity is to identify boundaries.  
% determining if boundaries/edges/connections are significant
This section summarizes our concept of human connectivity, but these concepts are not easily useful for decision makers if they cannot be expressed on a map \cite{wong15}.  Weiss' map of travel time to cities is a tangible example, but even it lacks discrete demarcations; for example, at what range is the travel time too far, and what demographic characteristics determine this value?  The fact that demarcations on a map are flexible, even arbitrary, yet also have a profound impact on how we interpret maps, and the results of technical spatial analysis, is known as the {\em modifiable areal unit problem} (MAUP), and it is a generalization of the {\em change of support problem} discussed in the previous subsection.  This section has presented concepts drawn from analysis of animal activity that argue for spatial demarcations based upon the perceptions of the people that will be impacted by decisions and policy.  Perception-based demarcation provides a principled method for solving the MAUP \cite{wong12}.  % notable, we've also argued for demarcations based only about homogeneity, so we're looking at two different ways of redrawing boundaries.
The final paragraphs of this section will examine technical methods for drawing discrete boundaries upon a map, and discuss some considerations that are relevant for policy makers when determining how to employ these methods.

Spatial divisions are discrete boundings that demarcate areas of space that, in some way, are homogenous; for example, areas within an administrative division are all under the same jurisdiction, and areas on either side of a contour line are all greater or lesser than some value.  Raster cells are also a kind of boundary, where resolution requires that all area within each cell have equal value.

Boundary detection is a set of methodologies for finding boundaries in some space.  Boundaries for categorical, non-overlapping, and spatially discrete areas are fairly simple to find; in these cases a boundary exists where categories are different between adjacent cells or areas.  Boundaries for continuously varying, non-discrete, or overlapping data are more difficult to identify.
Cluster detection is a type of boundary detection that finds related cases and bounds them within idealized shapes; model-based clustering is a standard among these types of methods \cite{fraley98, fraley02}.  {\em Moran's $I$} \cite{moran50} uses spatial autocorrelation to detect boundaries.  Wombling \cite{womble51} detects boundaries by identifying zones of rapid change.  The statistic {\em little $b$} \cite{jacquez08} identifies boundaries using a threshold in difference of values between two adjacent areas.  Methods are also available for measuring distance and identifying boundaries among polygons that overlap \cite{maruca02}.  %  The {\em little $b$} statistic is the least abstract method in terms of identifying arbitrary geographic boundaries.  The results of {\em little $b$} will for most policy applications be the easiest to interpret and most appropriate.

% hyperspectral (basically, a multivariate raster, see maruca2002, area-based tests for association between spatial patterns)

As mentioned in the last paragraph, finding boundaries around a group of raster cells representing categorical data is not difficult.  The raster package for the R project for statistical computing \cite{raster} defines a boundary around a group of raster cells as those cells that have adjacent cells with more than one class.  This simplicity is what motivates our proposal that the concepts of geodemography implemented with data downscaled to a raster provide a simple mechanism for policy and decision makers to construct a map that encodes their understanding of the world and the people they serve.  The primary input from a decision maker, in this arrangement, is information necessary to convert demographic data into discrete geodemographic classifications.  The next section provides motivating historical scenarios that illustrate the long term spatial impacts of policy.  After that, we provide a short example using census data from the Philippines that illustrate our conception of how decision makers can develop geodemographic classifications and apply them to derive spatial demarcations that are valuable for decision support.


% Transportation research has developed geovisualization of activity-travel through topography. Mei-Po Kwan outlined a series of geovisualization methods for human activity-travel in the Portland, Oregon metropolitan area \cite{kwan00}.  These models create a ``virtual topography'' in three dimensions of activity patterns. In doing so, they present a straightforward way to increase the effective impact of topography based on cultural factors and on perception, and offer a path into overlays and buffers which accurately display the real impedance to population mobility.


% graph theoretic interesting but not likely relevant.
% In ``Landscape Connectivity: A Graph-Theoretic Perspective'' \cite{urban01}, two different approaches to landscapes are typically in use. Vector-based maps use polygons (as in shapefiles in R) and lattices are ``rasterized'' (as in rasters in R) to represent the landscape as a grid, related to some mapping metric. A third approach to landscape is proposed, the graph. ``A graph represents a landscape as a set of nodes [...] connected to some degree by edges that join pairs of nodes functionally'' \cite{urban01}.  Graph theory is already used in Urban planning for transportation networks and problems of siting and routing applications, and in comptuer science, particularly in network optimization \cite{urban01}. These uses directly support applications in habitat fragmentation and landscape connectivity, as well as metapopulation theory.
% discuss connectivity models more generally as a basis for population topology; dig into strien17 and retrieve citations of early 'stepping stone' models.


\section{Policy Influence on Transboundary Social Condition}

\input{policy}


\section{Mapping social vulnerability} % and impacts for policy

% types of boundaries: geographic, administrative, cultural

The primary goal of geodemography and SEPMs is to identify types of people in the places they live, for the purpose of understanding the relationship between people and places, or implementing plans to reach or assist these people.  The organizations and agents who benefit from these tools and techniques are businesses, governing bodies, and decision makers.  % embellish.
% Identifying and bounding spaces that are homogenous or clustered upon metrics of interest is a way of providing parsimony, and enabling analysis and action.
The previous sections reviewed methods for classifying and bounding space, and the risks we take when we % recklessly draw boundaries, or
fail to appreciate the power that boundaries and policy have upon people's lives.  The following section applies these methods to demonstrate how demographic SEPMs can shed light upon the spatial distribution of social vulnerability around Metro Manila, Philippines.
% Such maps would benefit researchers who investigate and hypothesize on spatial demographic relationships, and policy and decision makers would also benefit from
% help anticipate humanitarian assistance needs following a disaster around a metro Manila.

% The example will analyze % immigration and
% social vulnerability in the area around Metro Manila, Philippines.  %, and provide spatially explicit characterization of the areas where immigrants reside.
% In addition to spatially explicit characterizations, the section will discretize spatial areas by social vulnerability 
% around neighborhoods of immigrants,
% and provide a simple and high-level demographic descriptions of these areas.

% Using the spatial characterizations, we will analyze the extent that migrants face additional risks from environmental disasters, and discuss how policy and decision makers can use such information to better serve this population and plan for eventual environmental events that will affect the population.

The example will analyze social vulnerability in the area around Metro Manila, Philippines.  The technical steps will proceed as follows:

\begin{enumerate}
\item Generate spatially explicit maps of immigration and indicators of vulnerabililty
\item Identify discrete boundaries that help summarize and demarcate the mapped information
\item Geodemographically describe the demarcations
\end{enumerate}

We will use dasymetric mapping to accomplish the first step.  The dasymetric model will bootstrap the spatial data, and use focal statistics to implement block-bootstrap by assigning each spatial variable it's average over a local neighborhood.
% supplement each cell with summaries of data in a surrounding neighborhood.
Focal statistics are calculated for a square block of 6.3 kilometers around each cell, taking a simple mean over the block; in this fashion the values at each cell represent a 6.3 kilometer block.  The predictor variables are population density, nighttime lights, travel-time to cities, and landcover.  % , along with squares and cubes of these variables.
Landscan \cite{landscan} supplies population density data.  VIIRS \cite{viirs} is the source for nighttime lights.  Weiss' travel-time to cities map quantifies the travel time data for our model.  The landcover data is built from VISNAV \cite{visnav} using only three landcover classes: urban, agricultural, and rice paddy.  The dependent variables are social vulnerability and migration, and are taken from the source zone aggregated values.  The source zones are municipalities--or level 2 administrative areas--and the target zones are raster cells.  IPUMS-International provides the census data for computing social vulnerability and migration, and the municipality polygon files for distributing the aggregated data to raster cells.  Raster cells that intersect two or more source zones receive their demographic characterization as an average of the source zones they intersect, weighted by the area of each intersection.

% The value of the dependent variable for each raster cell is estimated using a generalized linear model.  The social vulnerability index is a continuous linear value, so it is fit using a gaussian distribution--basically an OLS model.  The immigration data represents proportion of immigrants in each municipality, so it is fit using a binomial distribution--commonly known as a logistic regression model.  The following equation describes the from for each of these models.
% In short, a generalized linear model uses a {\em link function} to link non-continuous outputs to a linear model.  Non-continuous data includes counts and proportions, and are common in social and geographical modeling.  Essentially, the link function transforms non-continuous data to a continuous form for linear modeling, and then transforms the linear output back to the form of the original data using the inverse of the link function.  For example, proportional and binomial data is transformed from proportions or probabilities into log-odds using the logit link function; log-odds are linear and continuous and can be modeled with linear regression.

The value of the dependent variable for each raster cell is estimated using a generalized linear model.  The social vulnerability index is a continuous linear value, so it can be fit with a simple OLS model.  The following equation specifies the model.
% The immigration data represents proportion of immigrants in each municipality, so it is fit using a binomial distribution--commonly known as a logistic regression model.  The following equation describes the from for each of these models.

\[y=\alpha+\beta_1x_{pop}+\beta_2x_{pop}^2+\beta_3x_{urban}+\beta_4x_{ag}+\beta_5x_{paddy}+\beta_6x_{travel}+\beta_7x_{lights}\]

% The link function $g$ converts the non-continuous data to continuous linear form for modling.
The predictor varaibles are population $x_{pop}$, urban landcover $x_{urban}$, agricultural landcover $x_{ag}$, rice paddy landcover $x_{paddy}$, travel time to nearest city $x_{travel}$, and nighttime lights $x_{lights}$.  The values for each independent variable represent an average value over a neighborhood of about 6 kilometers, thus allowing for the effects of spatial dependence.

The distribution and uncertainty of the model is analyzed using a bootstrap.  The bootstrap resamples from among all raster cells in the data.  Because the model is large and can require substantial running time, we opted for 120 bootstrap resamples, although it is not uncommon for bootstrap models to use 10000 or more resamples.  Given the large size of the data, however, it is unlikely such a large number of resamples would reveal anything more than a more modest number.

Social vulnerability is calculated based on the index described in Ignacio et al \cite{ignacio15}.  The index is derived from a principal componenets analysis (PCA) of several variables taken to represent aspects of social vulnerability.  For simplicity, we retain only one component of the PCA analysis.  A higher value is associated with greater social vulnerability, and a lower value is associated with less.  A value of zero indicates average social vulnerability, but it shouldn't be taken to indicate absence of social vulnerability.  Our precise calculations of the index and extraction of the components is described in the supplementary source code materials.

% The model estimates two forms of immigration: internal migration between provinces, and international migration.  One model analyzes both types of migrants, while another analyzes only international migrants.
The model is fit to the entire land mass of the Philippines, however the following paragraphs constrain their analysis to Metro Manila.  Fitting the model to the full Philippines provides more data for making estimates, although it could be argued that fitting the model only to the area of analysis would yeild estimates that are more precise for that area.

% To facilitate interpretation of the model results, we wish to identify the peri-urban fringe of metro Manila.  The peri-urban fringe is a liminal space where the urban area begins to give way to the rural countryside.  The peri-urban fringe is not typically well defined, however we believe it may have an important relationship with both immigration and social vulnerability.


% \section{social vulnerability and immigration on the peri-urban fringe}

% Figures~\ref{fig:svi},~\ref{fig:maj}, and~\ref{fig:int} display the results of dasymetrically downscaling social vulnerability, internal migration between provinces, and international migration, respectively.  Associated with each map are uncertainty qauntifications, plotted in figures~\ref{fig:s_svi},~\ref{fig:s_maj}, and~\ref{fig:s_int}.

Figure~\ref{fig:svi} display the results of dasymetrically downscaling social vulnerability.  Associated with the model are uncertainty qauntifications, plotted in figure~\ref{fig:s_svi}.  Figure~\ref{fig:svi_choro} shows the choropleth mapping of the social vulnerability index for comparison.

It's notable that the social vulnerability mapping doesn't follow the municipality borders.  In any province with substantial deviation from average social vulnerability there tends to be both hotspots and coldspots.  Comparing the dasymetric with the choropleth map, the dasymetric map appears much more informative.  How do we know that the estimates of the dasymetric map are accurate?

The truth is that if we had the information necessary to assess the accuracy of this dasymetric map, we wouldn't need the dasymetric map in the first place.  The map in figure~\ref{fig:svi} is an estimate based upon our knowledge of how spatial features may correlate or even cause social vulnerability.  We can improve upon it with small-scale focused research that tests and establishes relationships between social vulnerability and geographic space, using geographic datasets with wide coverage that a dasymetric mapping algorithm can make use of.  The conclusion summarizes the messages of this article, as well as proposing potential future research that can improve our dasymetric maps of social vulnerability.


% \subsection{bounding the peri-urban fringe}

% geodemgraphic mapping (ala patchwork nation)
% dasymetric mapping
% combine them (vala!)


\section{Conclusion}

In this article we've discussed the importance of downscaling demographic data, and reviewed the basic methods for doing so.  Dasymetric mapping and interpolation methods both have long histories of downscaling population density data, however downscaling the demographic attributes along with the population appears to be less well understood.  We found that only recently have researchers begun to analyze the accuracy of downscaling demographic data, but the results and potential pitfalls of this method seem to remain relatively unexamined.

Due to the complexity and physical size of the data, and the many needs of its users, standard methods for geographically working with demographic data do not seem to exist.  Kriging and dasymetric mapping are two families of methods commonly employed to solve problems associated with geographically disaggregating or estimating data.  We've examined both families of methods and found that, for most users, regression weighted demographic mapping is an ideal method for geographically disaggregating demographic data.  RWDM is simple to apply and interpret, and it is generally effective and accurate.  An example problem demonstrated this method with the social vulnerabililty index, and showed that the results were, in general, prerable to using only the coarser choropleth maps.

It is often the case that the input data is more important than the technical details of the model, and this is certain to be true also of geographic disaggregation of demographic data.  For this reason, we believe that future research should focus on testing the relationship between geographical features and demographic attributes of interest.  This may often require field work or clever research designs that could not be applied at a broad geographic scale.  Small focused research efforts with limited geographic scope, however, can yield valuable and rigorously tested results that can support design decisions of models with broad or even global coverage.  For this research to be most effective it should test with data sources that are most valuable to models intended to have broad coverage.  Examples include land use data, climatology, satellite data, and crowdsourced data like OpenStreetMap.  A better understanding of how data sources like these relate to demographic variables will be valuable when designing models to spatially dissaggregate the data that characterizes human populations.


\bibliography{bootstrap,boundary_detection,dasymetric,dissertation,gis,kriging,microsimulation,pop_topo,regression_modeling}



\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
